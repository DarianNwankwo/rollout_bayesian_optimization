{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d22a8faf-a44c-4832-a2c0-4aed162be714",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfd792f-ebe1-414a-9ad3-7c690ed20ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../rollout.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df53765-9bcb-4af4-9450-6ec8c2608a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../testfns.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30efe8e4-065c-4f59-93a1-d9830d867fb5",
   "metadata": {},
   "source": [
    "### Psuedo-code for Rollout Bayesian Optimization\n",
    "1. Generate low-discrepancy sequence for Quasi-Monte Carlo\n",
    "2. Gather initial samples/experimental data\n",
    "3. Construct the ground truth surrogate model\n",
    "4. Setup hyperparameters for stochastic gradient descent\n",
    "5. While budget has not been exhausted\n",
    "<ol>\n",
    "    <li>\n",
    "        Construct a batch of samples for stochastic gradient descent. For each sample\n",
    "        <ol>\n",
    "            <li>Create a copy of the ground truth surrogate at the sample location and the pairwise perturbed surrogate.</li>\n",
    "            <li style=\"color: #f66\">Initialize our trajectory struct with the fantasized surrogate and fantisized perturbed surrogate and fantasy start location.</li>\n",
    "            <li>Perform rollout on the trajectory for $r$ steps $M_0$ times for Quasi-Monte Carlo integration.</li>\n",
    "            <li>Update values for $\\alpha$ and $\\nabla\\alpha$</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>Once SGD has converged, update sample location using update rule</li>\n",
    "    <li>Save location and value at location for each sample in batch.</li>\n",
    "    <li>Select the best sample from the batch and sample original process at new sample location.</li>\n",
    "    <li>Update surrogate model with value found at new sample location.</li>\n",
    "    <li>Repeat until budget is exhausted.</li>\n",
    "</ol>\n",
    "\n",
    "### Issues\n",
    "- Use control variates to see how they affect the rollout acquisition functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740e67f2-8b9f-4579-bb03-a72305371f58",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Probability of Improvement\n",
    "The probability of improvement (POI) is defined as follows:\n",
    "\n",
    "$$\n",
    "POI(x) = \\Phi\\left( \\frac{\\mu(x) - f^+ - \\xi}{\\sigma(x)} \\right)\n",
    "$$\n",
    "\n",
    "where $f^+$ denotes the best value (maximum) known, $\\mu(x)$ is the predictive mean at x, $\\sigma(x)$ is the predictive variance, $\\xi$ is our exploration parameter, and $\\Phi$ is the standard normal cumulative distribution function.\n",
    "<hr>\n",
    "\n",
    "#### Expected Improvement\n",
    "The expected improvement (EI) is defined as follows:\n",
    "\n",
    "$$\n",
    "EI(x) = (\\mu(x) - f^+ - \\xi)\\Phi\\left( \\frac{\\mu(x) - f^+ - \\xi}{\\sigma(x)} \\right) +\n",
    "        \\sigma(x)\\phi\\left(\\frac{\\mu(x) - f^+ - \\xi}{\\sigma(x)}\\right)\n",
    "$$\n",
    "\n",
    "where $\\phi$ is the standard normal probability density function.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb46be93-ff4d-430e-8235-6379a15d7a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "function ei(μ, σ, fbest)\n",
    "    z = (fbest - μ) / σ\n",
    "    Φz = Distributions.normcdf(z)\n",
    "    ϕz = Distributions.normpdf(z)\n",
    "    return σ*(z*Φz + ϕz)\n",
    "end\n",
    "\n",
    "function poi(μ, σ, fbest)\n",
    "    z = (fbest - μ) / σ\n",
    "    Φz = Distributions.normcdf(z)\n",
    "    return Φz\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9326ad1f-b1a8-41d6-98d4-f81de8dad559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "MAX_SGD_ITERS = 100\n",
    "BATCH_SIZE = 8\n",
    "HORIZON = 1\n",
    "MC_SAMPLES = 10\n",
    "BUDGET = 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e970b06e-5a3a-4fae-a41a-48bddde43d8e",
   "metadata": {},
   "source": [
    "### 1. Generate low-discrepancy sequence for Quasi-Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69b38c0-0e1e-4454-9dc4-364a4ccf4639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup toy problem\n",
    "# testfn = TestFunction(\n",
    "#     1, [0. 1.], [.5],\n",
    "#     x -> 0. + 1e-6*randn(),\n",
    "#     ∇x -> [0. + 1e-6*randn()]\n",
    "# )\n",
    "testfn = TestAckley(1)\n",
    "lbs, ubs = testfn.bounds[:,1], testfn.bounds[:,2]\n",
    "\n",
    "# Setup low discrepancy random number stream\n",
    "lds_rns = gen_low_discrepancy_sequence(MC_SAMPLES, testfn.dim, HORIZON+1);\n",
    "rns = randn(MC_SAMPLES, testfn.dim+1, HORIZON+1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eca1bf-7282-46b3-ba31-da6bb3a39db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tplot(testfn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4678e423-32e5-4b01-aae5-03d1aef751fc",
   "metadata": {},
   "source": [
    "### 2. Gather initial samples/experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb679e5-f73a-43f6-9f41-21acd5a0294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather initial samples/experimental data\n",
    "N, θ = 1, [.25]\n",
    "# X = [.15, .85]\n",
    "X = [21.7, -7.2, -17.5, -16.5] .- 6.\n",
    "# X = collect(-16.:2:16)\n",
    "X = reshape(X, 1, length(X))\n",
    "# ψ = kernel_matern52(θ);\n",
    "ψ = kernel_scale(kernel_matern52, [1., θ...]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755d46cc-e399-408e-9ab4-ebde68ab9343",
   "metadata": {},
   "source": [
    "### 3. Construct the ground truth surrogate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832624d6-1420-4c41-9b08-3603cca5311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sur = fit_surrogate(ψ, X, testfn.f);\n",
    "res = optimize_hypers_optim(sur, kernel_matern52)\n",
    "σ, ℓ = Optim.minimizer(res)\n",
    "ψ = kernel_scale(kernel_matern52, [σ, ℓ]);\n",
    "sur = fit_surrogate(ψ, X, testfn.f);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423926a8-dea6-48d0-a474-142f763c74b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = filter(x -> !(x in X), lbs[1]:.01:ubs[1])\n",
    "plot1D(sur; domain=domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2346c7-1ee3-47b7-89a7-70605ebe9a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot1DEI(sur, domain=domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8594aabc-2663-452a-a3bc-535a26bd3b4f",
   "metadata": {},
   "source": [
    "### 4. Setup hyperparameters for stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6cd22fd-2eac-4027-a0f9-b2f911161db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0e-5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the parameters of the optimizer\n",
    "λ = 0.01  # Learning rate\n",
    "β1 = 0.9  # First moment decay rate\n",
    "β2 = 0.999  # Second moment decay rate\n",
    "ϵ = 1e-8  # Epsilon value\n",
    "\n",
    "# Define the initial position and moment estimates\n",
    "m = zeros(testfn.dim)\n",
    "v = zeros(testfn.dim)\n",
    "\n",
    "ϵsgd = 1e-12\n",
    "grad_tol = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bb163c-c548-4c52-8e17-40baae9027e4",
   "metadata": {},
   "source": [
    "### 5. While budget has not been exhausted\n",
    "Each location in our minibatch is going to be our $x^0$ that serves as our deterministic start location. Then, we perform rollout from that point forward, computing several sample trajectories to then be averaged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4450c155-374a-4baf-aab8-90c13f8d2d95",
   "metadata": {},
   "source": [
    "We need a few mechanisms:\n",
    "* We shouldn't sample at locations that are near known locations in sur.X\n",
    "* We should perform the evaluations in parallel to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422ecbc6-1794-4fbc-bc61-b81a469151dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "minis = [minimum(sur.y) + sur.ymean]\n",
    "fbest = testfn.f(testfn.xopt...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b699bf-d6ff-4d01-a07f-c7378490fcbf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "∇αxs = []\n",
    "batch = []\n",
    "\n",
    "final_locations = []\n",
    "\n",
    "println(\"Beginning Bayesian Optimization Loop\")\n",
    "for b in 1:BUDGET\n",
    "    # Generate a batch of evaluation locations and filter out locations that are close\n",
    "    # to know sample locations\n",
    "    batch = generate_batch(BATCH_SIZE; lbs=lbs, ubs=ubs)\n",
    "    # batch = convert(Matrix{Float64}, filter(x -> !(x in sur.X), batch)')\n",
    "    \n",
    "    batch_evals = []\n",
    "    final_locations = []\n",
    "    \n",
    "    # This should be a parallel for loop\n",
    "    println(\"---------- BO Iteration #$b ----------\")\n",
    "    bndx = 1\n",
    "    for x0 in eachcol(batch)\n",
    "        try\n",
    "            x0 = convert(Vector{Float64}, x0)\n",
    "\n",
    "            αxs, ∇αxs = [], []\n",
    "            ∇αxs = [0., 1., 2.]\n",
    "\n",
    "            print(\"\\n(Batch #$bndx - $x0) Gradient Ascent Iteration Count: \")\n",
    "            # Run SGD until convergence\n",
    "            fprev, fnow = 0., 1.\n",
    "            for epoch in 1:MAX_SGD_ITERS\n",
    "                if mod(epoch, 25) == 0 print(\"|\") end\n",
    "                μx, ∇μx = simulate_trajectory(\n",
    "                    sur; mc_iters=MC_SAMPLES, rnstream=lds_rns, lbs=lbs, ubs=ubs, x0=x0, h=HORIZON\n",
    "                )\n",
    "\n",
    "                # Update gradient vector\n",
    "                push!(αxs, μx)\n",
    "                push!(∇αxs, first(∇μx))\n",
    "\n",
    "                fprev = fnow\n",
    "                fnow = μx\n",
    "\n",
    "                # Update x0 based on gradient computation\n",
    "                x0, m, v = update_x_adam(x0; ∇g=-∇μx, λ=λ, β1=β1, β2=β2, ϵ=ϵ, m=m, v=v, lbs=lbs, ubs=ubs)\n",
    "                # x0 = update_x(x0; λ=λ, ∇g=∇μx, lbs=lbs, ubs=ubs)\n",
    "\n",
    "                if abs(fnow - fprev) < ϵsgd || norm(∇μx) < grad_tol\n",
    "                    println(\"\\nConverged after $epoch epochs\")\n",
    "                    # println(\"abs(fnow - fprev): $(abs(fnow - fprev)) - fnow: $fnow - fprev: $fprev\")\n",
    "                    break\n",
    "                end\n",
    "\n",
    "            end\n",
    "\n",
    "            push!(batch_evals, αxs[end])\n",
    "            push!(final_locations, x0)\n",
    "            bndx += 1\n",
    "        catch e\n",
    "            bndx += 1\n",
    "            println(e)\n",
    "        end\n",
    "    end\n",
    "    # Iterate over batch for best response and sample original process afterwards\n",
    "    if length(batch_evals) > 0\n",
    "        println()\n",
    "        [println(\"α($(pair[1])) = $(pair[2])\") for pair in zip(final_locations, batch_evals)]\n",
    "        ndx = argmax(batch_evals)\n",
    "        xnew = final_locations[ndx]\n",
    "\n",
    "        # Sample original process at x0\n",
    "        println(\"\\nFinal xnew: $xnew\")\n",
    "        println(\"--------------------------------------\\n\")\n",
    "        res = optimize_hypers_optim(sur, kernel_matern52)\n",
    "        σ, ℓ = Optim.minimizer(res)\n",
    "        println(\"Learned Kernel Hyperparameters: σ=$σ -- ℓ=$ℓ\")\n",
    "        ψ = kernel_scale(kernel_matern52, [σ, ℓ]);\n",
    "        recover_y = sur.y .+ sur.ymean\n",
    "        sur = fit_surrogate(\n",
    "            ψ,\n",
    "            hcat(sur.X, xnew),\n",
    "            vcat(recover_y, testfn.f(xnew))\n",
    "        )\n",
    "        \n",
    "        push!(minis, minimum(sur.y) + sur.ymean)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfea303-c85d-49f5-8962-0d7bf899eef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = filter(x -> !(x in sur.X), lbs[1]:.01:ubs[1])\n",
    "plot1D(sur; domain=domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cb7a93-416c-4503-ae0b-bcb5f07f4ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_mini = minis[1]\n",
    "gap = (init_mini .- minis) / (init_mini - fbest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1902a0-a972-47e8-b34a-8346796cd346",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47317816-8149-4863-9329-60556ab65bc3",
   "metadata": {},
   "source": [
    "### Parallel Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77c675da-b97a-4724-81cb-2202696154ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere function simulate_trajectory2(sur::RBFsurrogate; x0, mc_iters, rnstream, lbs, ubs, h)\n",
    "    αxi, ∇αxi = 0., zeros(size(sur.X, 1))\n",
    "\n",
    "    for sample in 1:mc_iters\n",
    "        fsur = Base.deepcopy(sur)\n",
    "        fantasy_ndx = size(fsur.X, 2) + 1\n",
    "\n",
    "        # Rollout trajectory\n",
    "        T = Trajectory(fsur, x0, fantasy_ndx; h=h, fopt=minimum(sur.y))\n",
    "        rollout!(T, lbs, ubs; rnstream=rnstream[sample,:,:])\n",
    "\n",
    "        # Evaluate rolled out trajectory\n",
    "        αxi += α(T)\n",
    "        ∇αxi .+= ∇α(T)\n",
    "    end\n",
    "\n",
    "    # Average trajectories MC simulation\n",
    "    μx = αxi / mc_iters\n",
    "    ∇μx = ∇αxi / mc_iters\n",
    "\n",
    "    return μx, ∇μx\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c710e9c3-d65b-40a1-9ed7-9a50d0f2cdca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Bayesian Optimization Loop\n",
      "---------- BO Iteration #1 ----------\n",
      "Task (runnable) @0x00007f473d53b3d0\n",
      "      From worker 2:\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91m\u001b[1mUnhandled Task \u001b[22m\u001b[39m\u001b[91m\u001b[1mERROR: \u001b[22m\u001b[39mOn worker 2:\n",
      "UndefVarError: LazyStruct not defined\n",
      "Stacktrace:\n",
      "  [1] \u001b[0m\u001b[1meval\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m~/Development/rollout_bayesian_optimization/\u001b[39m\u001b[90m\u001b[4mradial_basis_surrogates_update.jl:119\u001b[24m\u001b[39m\n",
      "  [2] \u001b[0m\u001b[1meval\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m~/Development/rollout_bayesian_optimization/\u001b[39m\u001b[90m\u001b[4mradial_basis_surrogates_update.jl:238\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
      "  [3] \u001b[0m\u001b[1mRBFsurrogate\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m~/Development/rollout_bayesian_optimization/\u001b[39m\u001b[90m\u001b[4mradial_basis_surrogates_update.jl:239\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
      "  [4] \u001b[0m\u001b[1m#gp_draw#245\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m~/Development/rollout_bayesian_optimization/\u001b[39m\u001b[90m\u001b[4mradial_basis_surrogates_update.jl:242\u001b[24m\u001b[39m\n",
      "  [5] \u001b[0m\u001b[1m#rollout!#294\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m~/Development/rollout_bayesian_optimization/\u001b[39m\u001b[90m\u001b[4mrollout.jl:27\u001b[24m\u001b[39m\n",
      "  [6] \u001b[0m\u001b[1m#simulate_trajectory2#298\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[14]:10\u001b[24m\u001b[39m\n",
      "  [7] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mIn[15]:23\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
      "  [8] \u001b[0m\u001b[1m#643\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m~/packages/julias/julia-1.8/share/julia/stdlib/v1.8/Distributed/src/\u001b[39m\u001b[90m\u001b[4mmacros.jl:303\u001b[24m\u001b[39m\n",
      "  [9] \u001b[0m\u001b[1m#178\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m~/packages/julias/julia-1.8/share/julia/stdlib/v1.8/Distributed/src/\u001b[39m\u001b[90m\u001b[4mmacros.jl:83\u001b[24m\u001b[39m\n",
      " [10] \u001b[0m\u001b[1m#invokelatest#2\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4messentials.jl:729\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
      " [11] \u001b[0m\u001b[1minvokelatest\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4messentials.jl:726\u001b[24m\u001b[39m\n",
      " [12] \u001b[0m\u001b[1m#107\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m~/packages/julias/julia-1.8/share/julia/stdlib/v1.8/Distributed/src/\u001b[39m\u001b[90m\u001b[4mprocess_messages.jl:281\u001b[24m\u001b[39m\n",
      " [13] \u001b[0m\u001b[1mrun_work_thunk\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m~/packages/julias/julia-1.8/share/julia/stdlib/v1.8/Distributed/src/\u001b[39m\u001b[90m\u001b[4mprocess_messages.jl:70\u001b[24m\u001b[39m\n",
      " [14] \u001b[0m\u001b[1mrun_work_thunk\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m~/packages/julias/julia-1.8/share/julia/stdlib/v1.8/Distributed/src/\u001b[39m\u001b[90m\u001b[4mprocess_messages.jl:79\u001b[24m\u001b[39m\n",
      " [15] \u001b[0m\u001b[1m#100\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mtask.jl:484\u001b[24m\u001b[39m\n",
      "Stacktrace:\n",
      " [1] \u001b[0m\u001b[1msync_end\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mc\u001b[39m::\u001b[0mChannel\u001b[90m{Any}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n",
      "\u001b[90m   @ \u001b[39m\u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mtask.jl:436\u001b[24m\u001b[39m\n",
      " [2] \u001b[0m\u001b[1m(::Distributed.var\"#177#179\"{var\"#643#644\"{Vector{Float64}}, UnitRange{Int64}})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[0m\u001b[1m)\u001b[22m\n",
      "\u001b[90m   @ \u001b[39m\u001b[35mDistributed\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mtask.jl:455\u001b[24m\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "println(\"Beginning Bayesian Optimization Loop\")\n",
    "for b in 1:BUDGET\n",
    "    # Generate a batch of evaluation locations and filter out locations that are close\n",
    "    # to know sample locations\n",
    "    # batch = generate_batch(BATCH_SIZE; lbs=lbs, ubs=ubs)\n",
    "    batch = generate_batch(1; lbs=lbs, ubs=ubs)\n",
    "\n",
    "    \n",
    "    # This should be a parallel for loop\n",
    "    println(\"---------- BO Iteration #$b ----------\")\n",
    "    batch_results = @distributed for j = 1:size(batch, 2)\n",
    "        # try\n",
    "        x0 = batch[:, j]\n",
    "\n",
    "        αxs, ∇αxs = [], []\n",
    "        ∇αxs = [0., 1., 2.]\n",
    "\n",
    "        print(\"\\n(Batch #$j - $x0) Gradient Ascent Iteration Count: \")\n",
    "        # Run SGD until convergence\n",
    "        fprev, fnow = 0., 1.\n",
    "        for epoch in 1:MAX_SGD_ITERS\n",
    "            if mod(epoch, 25) == 0 print(\"|\") end\n",
    "            μx, ∇μx = simulate_trajectory2(\n",
    "                sur; mc_iters=MC_SAMPLES, rnstream=lds_rns, lbs=lbs, ubs=ubs, x0=x0, h=HORIZON\n",
    "            )\n",
    "\n",
    "            # Update gradient vector\n",
    "            push!(αxs, μx)\n",
    "            push!(∇αxs, first(∇μx))\n",
    "\n",
    "            fprev = fnow\n",
    "            fnow = μx\n",
    "\n",
    "            # Update x0 based on gradient computation\n",
    "            x0, m, v = update_x_adam(x0; ∇g=-∇μx, λ=λ, β1=β1, β2=β2, ϵ=ϵ, m=m, v=v, lbs=lbs, ubs=ubs)\n",
    "            # x0 = update_x(x0; λ=λ, ∇g=∇μx, lbs=lbs, ubs=ubs)\n",
    "\n",
    "            if abs(fnow - fprev) < ϵsgd || norm(∇μx) < grad_tol\n",
    "                println(\"\\nConverged after $epoch epochs\")\n",
    "                # println(\"abs(fnow - fprev): $(abs(fnow - fprev)) - fnow: $fnow - fprev: $fprev\")\n",
    "                break\n",
    "            end\n",
    "\n",
    "        end\n",
    "\n",
    "            return (x0, αxs[end])\n",
    "        # catch e\n",
    "        #     println(e)\n",
    "        # end\n",
    "    end\n",
    "    \n",
    "    println(batch_results)\n",
    "    # batch_results = vcat(batch_results...)\n",
    "    \n",
    "#     batch_evals = [br[2] for br in batch_results]\n",
    "#     final_locations = [br[1] for br in batch_results]\n",
    "    \n",
    "#     # Iterate over batch for best response and sample original process afterwards\n",
    "#     if length(batch_evals) > 0\n",
    "#         println()\n",
    "#         [println(\"α($(pair[1])) = $(pair[2])\") for pair in zip(final_locations, batch_evals)]\n",
    "#         ndx = argmax(batch_evals)\n",
    "#         xnew = final_locations[ndx]\n",
    "\n",
    "#         # Sample original process at x0\n",
    "#         println(\"\\nFinal xnew: $xnew\")\n",
    "#         println(\"--------------------------------------\\n\")\n",
    "#         res = optimize_hypers_optim(sur, kernel_matern52)\n",
    "#         σ, ℓ = Optim.minimizer(res)\n",
    "#         println(\"Learned Kernel Hyperparameters: σ=$σ -- ℓ=$ℓ\")\n",
    "#         ψ = kernel_scale(kernel_matern52, [σ, ℓ]);\n",
    "#         recover_y = sur.y .+ sur.ymean\n",
    "#         sur = fit_surrogate(\n",
    "#             ψ,\n",
    "#             hcat(sur.X, xnew),\n",
    "#             vcat(recover_y, testfn.f(xnew))\n",
    "#         )\n",
    "        \n",
    "#         push!(minis, minimum(sur.y) + sur.ymean)\n",
    "#     end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04b2827-bee3-4d9d-9152-95af3353e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your custom data structure\n",
    "struct MyCustomType\n",
    "    a::Int\n",
    "    b::Float64\n",
    "end\n",
    "\n",
    "# evaluate the definition of your custom type on all worker processes\n",
    "@everywhere struct MyCustomType\n",
    "    a::Int\n",
    "    b::Float64\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb0dca3-8de4-4d0a-9754-d296f7a6425e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
